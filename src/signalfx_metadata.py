#!/usr/bin/python

import array
import binascii
import fcntl
import os
import os.path
import platform
import random
import re
import signal
import socket
import string
import struct
import subprocess
import sys
import threading
import time
import zlib
from urlparse import urlparse

import psutil

import collectd_dogstatsd

try:
    import urllib.request as urllib2
except ImportError:
    import urllib2

try:
    import json
except ImportError:
    import simplejson as json

try:
    str.decode("ami3?")
    bytes = str
except:
    pass

try:
    import collectd
    import logging

    logging.basicConfig(level=logging.INFO)
except ImportError:
    try:
        import dummy_collectd as collectd
    except:
        pass

PLUGIN_NAME = 'signalfx-metadata'
API_TOKEN = ""
TIMEOUT = 3
POST_URL = "https://ingest.signalfx.com/v1/collectd"
VERSION = "0.0.17"
NOTIFY_LEVEL = -1
HOST_TYPE_INSTANCE = "host-meta-data"
TOP_TYPE_INSTANCE = "top-info"
TYPE = "objects"
NEXT_METADATA_SEND = 0
NEXT_METADATA_SEND_INTERVAL = \
    [1, 60, 3600 + random.randint(0, 60), 86400 + random.randint(0, 600)]
LAST = 0
AWS = True
AWS_SET = False
PROCESS_INFO = True
DPM = False
UTILIZATION = True
INTERVAL = 10
HOST = ""
UP = time.time()
DEBUG = False

RESPONSE_LOCK = threading.Lock()
METRIC_LOCK = threading.Lock()
MAX_RESPONSE = 0
RESPONSE_ERRORS = 0
DATAPOINT_COUNT = {}

CPU_HISTORY = {}
CPU_DONE = []
CPU_LAST = {}

MEMORY_HISTORY = {}
MEMORY_DONE = []
MAX_MEMORY_LENGTH = 0
DF_HISTORY = {}
DF_DONE = []
MAX_DF_LENGTH = 0
DISK_IO_HISTORY = {}
DISK_IO_DONE = []
MAX_DISK_IO_LENGTH = 0
NETWORK_HISTORY = {}
NETWORK_DONE = []
MAX_NETWORK_LENGTH = 0

DOGSTATSD_INSTANCE = collectd_dogstatsd.DogstatsDCollectD(collectd)


class LargeNotif:
    """
    Used because the Python plugin supplied notification does not provide
    us with enough space
    """
    host = ""
    message = ""
    plugin = PLUGIN_NAME
    plugin_instance = ""
    severity = 4
    time = 0
    type = TYPE
    type_instance = ""

    def __init__(self, message, type_instance="", plugin_instance=""):
        self.plugin_instance = plugin_instance
        self.type_instance = type_instance
        self.message = message
        self.host = HOST

    def __repr__(self):
        return 'PUTNOTIF %s/%s-%s/%s-%s %s' % (self.host, self.plugin,
                                               self.plugin_instance,
                                               self.type, self.type_instance,
                                               self.message)


def debug(param):
    """ debug messages and understand if we're in collectd or a program """
    if DEBUG:
        if __name__ != '__main__':
            collectd.info("%s: DEBUG %s" % (PLUGIN_NAME, param))
        else:
            sys.stderr.write("%s\n" % param)


def log(param):
    """ log messages and understand if we're in collectd or a program """
    if __name__ != '__main__':
        collectd.info("%s: %s" % (PLUGIN_NAME, param))
    else:
        sys.stderr.write("%s\n" % param)


def plugin_config(conf):
    """
    :param conf:
      https://collectd.org/documentation/manpages/collectd-python.5.shtml#config

    Parse the config object for config parameters:
      ProcessInfo: true or false, whether or not to collect process
        information. Default is true.
      Notifications: true or false, whether or not to emit notifications
      if Notifications is true:
        URL: where to POST the notifications to
        Token: what auth to send along
        Timeout: timeout for the POST
        NotifyLevel: what is the lowest level of notification to emit.
          Default is to only emit notifications generated by this plugin
    """

    DOGSTATSD_INSTANCE.config.configure_callback(conf)

    for kv in conf.children:
        if kv.key == 'Notifications':
            if kv.values[0]:
                log("sending collectd notifications")
                collectd.register_notification(receive_notifications)
        elif kv.key == 'ProcessInfo':
            global PROCESS_INFO
            PROCESS_INFO = kv.values[0]
        elif kv.key == 'Utilization':
            global UTILIZATION
            UTILIZATION = kv.values[0]
        elif kv.key == 'DPM':
            global DPM
            DPM = kv.values[0]
        elif kv.key == 'Verbose':
            global DEBUG
            DEBUG = kv.values[0]
            log('setting verbose to %s' % DEBUG)
        elif kv.key == 'URL':
            global POST_URL
            POST_URL = kv.values[0]
        elif kv.key == 'Token':
            global API_TOKEN
            API_TOKEN = kv.values[0]
        elif kv.key == 'Timeout':
            global TIMEOUT
            TIMEOUT = int(kv.values[0])
        elif kv.key == 'Interval':
            global INTERVAL
            INTERVAL = int(kv.values[0])
        elif kv.key == 'NotifyLevel':
            global NOTIFY_LEVEL
            if string.lower(kv.values[0]) == "okay":
                NOTIFY_LEVEL = 4
            elif string.lower(kv.values[0]) == "warning":
                NOTIFY_LEVEL = 2
            elif string.lower(kv.values[0]) == "failure":
                NOTIFY_LEVEL = 1

    if DPM or UTILIZATION:
        collectd.register_write(receive_datapoint)

    collectd.register_read(send, INTERVAL)
    set_aws_url(get_aws_info())


def compact(thing):
    return json.dumps(thing, separators=(',', ':'))


STARTING = {}


def emit_total(DONE, field, metric):
    """
    emit summary for field as metric

    :param DONE: a list of completed metrics
    :param field: the field to sum over
    :param metric: the metric to be emitted as
    :return: None
    """
    for t, m in DONE:
        try:
            total = sum(sum(v[field]) for v in m.values())
            put_val("summation", "", [total, metric], t=t, i=1)
            debug("%s summation %s %s %s" % (t, metric, total, m))
        except:
            t, e = sys.exc_info()[:2]
            debug("trying to emit %s failed %s %s %s" % (t, metric, e, m))


def emit_utilization(used, total, metric, plugin_instance="utilization",
                     t=0.0, obj=None):
    """
    emit a utilization metric

    :param used: how much has been used
    :param total: total amount
    :param metric: metric to be emitted as
    :param plugin_instance:  plugin_instance to be emitted as
    :param t: time to emit, default is 0.0 meaning now
    :param obj: debug passthrough
    :return:
    """
    if total == 0:
        percent = 0
    else:
        percent = 1.0 * used / total * 100
    if percent < 0:
        log("percent <= 0 %s %s %s %s %s" %
            (used, total, metric, plugin_instance, obj))
        return percent
    if percent > 100:
        log("percent > 100 %s %s %s %s %s" %
            (used, total, metric, plugin_instance, obj))
        return percent
    put_val(plugin_instance, "", [percent, metric], t=t, i=1)
    debug("%s %s %s" % (t, metric, percent))
    return percent


def emit_disk_total():
    """
    emits a total for all disk iops that have occurred.

    :return: None
    """
    global DISK_IO_DONE
    emit_total(DISK_IO_DONE, "disk_ops", "disk_ops.total")
    DISK_IO_DONE = []


def emit_network_total():
    """
    emits a total for all network bytes that have occurred.

    :return: None
    """
    global NETWORK_DONE
    emit_total(NETWORK_DONE, "if_octets", "network.total")
    NETWORK_DONE = []


def emit_df_utilization():
    """
    emit disk utilization metrics when we've seen all the disk metrics we need
    Note that this emits one utilization metric for each mount point and a
    total utilization.

    :return: None
    """
    used_total = 0
    total_total = 0
    global DF_DONE
    if DF_DONE:
        debug(DF_DONE)
    for t, m in DF_DONE:
        for plugin_instance, v in m.iteritems():
            try:
                used = v["df_complex.used"][0]
                free = v["df_complex.free"][0]
                total = used + free
                emit_utilization(used, total,
                                 "disk.utilization", plugin_instance,
                                 t=t, obj=v)
                total_total += total
                used_total += used
            except:
                t, e = sys.exc_info()[:2]
                debug("trying to emit df utilization failed %s %s %s" %
                      (e, plugin_instance, v))
        if used_total:
            emit_utilization(used_total, total_total,
                             "disk.summary_utilization", t=t)
    DF_DONE = []


def emit_memory_utilization():
    """
    emit memory utilization metric when we've seen all the memory metrics we
    need

    :return: None
    """
    global MEMORY_DONE
    if MEMORY_DONE:
        debug(MEMORY_DONE)
    for t, m in MEMORY_DONE:
        try:
            total = sum(c[0] for c in m.values())
            used = total - m["memory.free"][0]
            emit_utilization(used, total, "memory.utilization", t=t)
        except:
            t, e = sys.exc_info()[:2]
            debug("trying to emit memory utilization failed %s %s" % (e, m))
    MEMORY_DONE = []


def emit_cpu_utilization():
    """
    emit cpu utilization metric when we've seen all the cpu aggregation metrics
    we need

    :return: None
    """
    global CPU_DONE, CPU_LAST
    if CPU_DONE:
        debug(CPU_DONE)
    for t, m in CPU_DONE:
        try:
            if len(CPU_LAST) < 8:
                CPU_LAST.update(m)
                return

            old_total = sum(c[0] for c in CPU_LAST.values())
            old_idle = CPU_LAST.get("cpu.idle", [0])[0]
            old_used = old_total - old_idle

            CPU_LAST.update(m)

            total = sum(c[0] for c in CPU_LAST.values())
            idle = CPU_LAST.get("cpu.idle", [0])[0]
            used = total - idle

            used_diff = used - old_used
            total_diff = total - old_total
            if used_diff < 0 or total_diff < 0:
                log("t %s used %s total %s old used %s old total %s" % (
                    t, used, total, old_used, old_total))
                break
            if used_diff == 0 and total_diff == 0:
                debug("zeros %s %s" % (CPU_LAST, m))
                break

            emit_utilization(used_diff, total_diff,
                             "cpu.utilization", t=t, obj=(t, m))
        except:
            t, e = sys.exc_info()[:2]
            debug("trying to emit cpu utilization failed %s %s %s" % (t, e, m))
    CPU_DONE = []


def emit_utilizations():
    """
    Emit all utilizations, this is called at a frequency of 1 to send any
    metrics that may be collectd at any rate.  Most of these will be noops.

    :return: None
    """
    with METRIC_LOCK:
        emit_cpu_utilization()
        emit_df_utilization()
        emit_memory_utilization()
        emit_disk_total()
        emit_network_total()


def send():
    """
    Sends datapoints and metadata events

    dimensions existing
    """

    send_datapoints()
    DOGSTATSD_INSTANCE.read_callback()

    send_top()

    # race condition with host dimension existing
    # don't send metadata on initial iteration, but on second
    # send it then on minute later, then one hour, then one day, then once a
    # day from then on but off by a fudge factor
    global NEXT_METADATA_SEND
    if NEXT_METADATA_SEND == 0:
        NEXT_METADATA_SEND = time.time() + NEXT_METADATA_SEND_INTERVAL.pop(0)
        log("waiting one interval before sending notifications")
    if NEXT_METADATA_SEND < time.time():
        send_notifications()
        if len(NEXT_METADATA_SEND_INTERVAL) > 1:
            NEXT_METADATA_SEND = \
                time.time() + NEXT_METADATA_SEND_INTERVAL.pop(0)
        else:
            NEXT_METADATA_SEND = time.time() + NEXT_METADATA_SEND_INTERVAL[0]
        log("till next metadata " + str(NEXT_METADATA_SEND - time.time()))

    global LAST
    LAST = time.time()


def all_interfaces():
    """
    source # http://bit.ly/1K8LIFH
    could use netifaces but want to package as little code as possible

    :return: all ip addresses by interface
    """
    is_64bits = struct.calcsize("P") == 8
    struct_size = 32
    if is_64bits:
        struct_size = 40
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    max_possible = 8  # initial value
    while True:
        _bytes = max_possible * struct_size
        names = array.array('B')
        for i in range(0, _bytes):
            names.append(0)
        outbytes = struct.unpack('iL', fcntl.ioctl(
            s.fileno(),
            0x8912,  # SIOCGIFCONF
            struct.pack('iL', _bytes, names.buffer_info()[0])
        ))[0]
        if outbytes == _bytes:
            max_possible *= 2
        else:
            break
    namestr = names.tostring()
    ifaces = []
    for i in range(0, outbytes, struct_size):
        iface_name = bytes.decode(namestr[i:i + 16]).split('\0', 1)[0]
        iface_addr = socket.inet_ntoa(namestr[i + 20:i + 24])
        ifaces.append((iface_name, iface_addr))

    return ifaces


def get_interfaces(host_info={}):
    """populate host_info with the ipaddress and fqdn for each interface"""
    interfaces = {}
    for interface, ipaddress in all_interfaces():
        if ipaddress == "127.0.0.1":
            continue
        interfaces[interface] = \
            (ipaddress, socket.getfqdn(ipaddress))
    host_info["sf_host_interfaces"] = compact(interfaces)


def get_cpu_info(host_info={}):
    """populate host_info with cpu information"""
    with open("/proc/cpuinfo") as f:
        nb_cpu = 0
        nb_cores = 0
        nb_units = 0
        for p in f.readlines():
            if ':' in p:
                x, y = map(lambda x: x.strip(), p.split(':', 1))
                if x.startswith("physical id"):
                    if nb_cpu < int(y):
                        nb_cpu = int(y)
                if x.startswith("cpu cores"):
                    if nb_cores < int(y):
                        nb_cores = int(y)
                if x.startswith("processor"):
                    if nb_units < int(y):
                        nb_units = int(y)
                if x.startswith("model name"):
                    model = y

        nb_cpu += 1
        nb_units += 1
        host_info["host_cpu_model"] = model
        host_info["host_physical_cpus"] = str(nb_cpu)
        host_info["host_cpu_cores"] = str(nb_cores)
        host_info["host_logical_cpus"] = str(nb_units)

    return host_info


def get_kernel_info(host_info={}):
    """
    gets kernal information from platform, relies on the restore_sigchld
    call above to work on python 2.6
    """
    try:
        host_info["host_kernel_name"] = platform.system()
        host_info["host_kernel_release"] = platform.release()
        host_info["host_kernel_version"] = platform.version()
        host_info["host_machine"] = platform.machine()
        host_info["host_processor"] = platform.processor()
    except:
        log("still seeing exception in platform module")

    return host_info


def get_aws_info(host_info={}):
    """
    call into aws to get some information about the instance, timeout really
    small for non aws systems and only try the once per startup
    """
    global AWS
    if not AWS:
        return host_info

    url = "http://169.254.169.254/latest/dynamic/instance-identity/document"
    try:
        req = urllib2.Request(url)
        response = urllib2.urlopen(req, timeout=0.1)
        identity = json.loads(response.read())
        want = {
            'availability_zone': 'availabilityZone',
            'instance_type': 'instanceType',
            'instance_id': 'instanceId',
            'image_id': 'imageId',
            'account_id': 'accountId',
            'region': 'region',
            'architecture': 'architecture',
        }
        for k, v in iter(want.items()):
            host_info["aws_" + k] = identity[v]
    except:
        log("not an aws box")
        AWS = False

    return host_info


def set_aws_url(host_info):
    global AWS_SET, POST_URL
    if AWS and not AWS_SET:
        result = urlparse(POST_URL)
        if "sfxdim_AWSUniqueId" not in result.query:
            dim = "sfxdim_AWSUniqueId=%s_%s_%s" % \
                  (host_info["aws_instance_id"],
                   host_info["aws_region"], host_info["aws_account_id"])
            if result.query:
                POST_URL += "&%s" % dim
            else:
                POST_URL += "?%s" % dim
            log("adding %s to post_url for uniqueness" % dim)
        AWS_SET = True


def popen(command):
    """ using subprocess instead of check_output for 2.6 comparability """
    output = subprocess.Popen(command, stdout=subprocess.PIPE).communicate()[0]
    return output.strip()


def get_collectd_version(host_info={}):
    """
    exec the pid (which will be collectd) with help and parse the help
    message for the version information
    """
    host_info["host_collectd_version"] = "UNKNOWN"
    try:
        output = popen(["/proc/self/exe", "-h"])
        regexed = re.search("collectd (.*), http://collectd.org/",
                            output.decode())
        if regexed:
            host_info["host_collectd_version"] = regexed.groups()[0]
    except Exception:
        t, e = sys.exc_info()[:2]
        log("trying to parse collectd version failed %s" % e)

    return host_info


def getLsbRelease(host_info={}):
    if os.path.isfile("/etc/lsb-release"):
        with open("/etc/lsb-release") as f:
            for line in f.readlines():
                regexed = re.search('DISTRIB_DESCRIPTION="(.*)"', line)
                if regexed:
                    host_info["host_linux_version"] = regexed.groups()[0]
                    return host_info["host_linux_version"]


def getOsRelease(host_info={}):
    if os.path.isfile("/etc/os-release"):
        with open("/etc/os-release") as f:
            for line in f.readlines():
                regexed = re.search('PRETTY_NAME="(.*)"', line)
                if regexed:
                    host_info["host_linux_version"] = regexed.groups()[0]
                    return host_info["host_linux_version"]


def getCentos(host_info={}):
    for file in ["/etc/centos-release", "/etc/redhat-release",
                 "/etc/system-release"]:
        if os.path.isfile(file):
            with open(file) as f:
                line = f.read()
                host_info["host_linux_version"] = line.strip()
                return host_info["host_linux_version"]


def get_linux_version(host_info={}):
    """
    read a variety of files to figure out linux version
    """

    for f in [getLsbRelease, getOsRelease, getCentos]:
        if f(host_info):
            return

    host_info["host_linux_version"] = "UNKNOWN"
    return host_info


def parse_bytes(possible_bytes):
    """bytes can be compressed with suffixes but we want real numbers in kb"""
    try:
        return int(possible_bytes)
    except:
        if possible_bytes[-1].lower() == 'm':
            return int(float(possible_bytes[:-1]) * 1024)
        if possible_bytes[-1].lower() == 'g':
            return int(float(possible_bytes[:-1]) * 1024 ** 2)
        if possible_bytes[-1].lower() == 't':
            return int(float(possible_bytes[:-1]) * 1024 ** 3)
        if possible_bytes[-1].lower() == 'p':
            return int(float(possible_bytes[:-1]) * 1024 ** 4)
        if possible_bytes[-1].lower() == 'e':
            return int(float(possible_bytes[:-1]) * 1024 ** 5)


def parse_priority(priority):
    """
    priority can sometimes be "rt" for real time, make that 99, the highest
    """
    try:
        return int(priority)
    except:
        return 99


def to_time(secs):
    minutes = int(secs / 60)
    seconds = secs % 60.0
    sec = int(seconds)
    dec = int((seconds - sec) * 100)
    return "%02d:%02d.%02d" % (minutes, sec, dec)


def read_proc_file(pid, file, field=None):
    with open("/proc/%s/%s" % (pid, file)) as f:
        if not field:
            return f.read().strip()
        for x in f.readlines():
            if x.startswith(field):
                return x.split(":")[1].strip()


def get_priority(pid):
    val = read_proc_file(pid, "sched", "prio")
    val = int(val) - 100
    if val < 0:
        val = 99
    return val


def get_command(p):
    val = " ".join(p.cmdline())
    if not val:
        val = read_proc_file(p.pid, "status", "Name")
        val = "[%s]" % val
    return val


def get_nice(p):
    val = read_proc_file(p.pid, "stat")
    return val.split()[18]


def send_top():
    """
    Parse top unless told not to
    filter out any zeros and common values to save space send it directly
    without going through collectd mechanisms because it is too large
    """
    if not PROCESS_INFO:
        return

    status_map = {
        "sleeping": "S",
        "uninterruptible sleep": "D",
        "running": "R",
        "traced": "T",
        "stopped": "T",
        "zombie": "Z",
    }

    # send version up with the values
    response = {"v": VERSION}
    top = {}
    for p in psutil.process_iter():
        try:
            top[p.pid] = [
                p.username(),  # user
                get_priority(p.pid),  # priority
                get_nice(p),  # nice value, numerical
                p.memory_info_ex()[1] / 1024,  # virtual memory size in kb int
                p.memory_info_ex()[0] / 1024,  # resident memory size in kb int
                p.memory_info_ex()[2] / 1024,  # shared memory size in kb int
                status_map.get(p.status(), "D"),  # process status
                p.cpu_percent(),  # % cpu, float
                p.memory_percent(),  # % mem, float
                to_time(p.cpu_times().system + p.cpu_times().user),  # cpu time
                get_command(p)  # command
            ]
        except Exception:
            # eat exceptions here because they're very noisy
            pass

    s = compact(top)
    compressed = zlib.compress(s.encode("utf-8"))
    base64 = binascii.b2a_base64(compressed)
    response["t"] = base64.decode("utf-8")
    response_json = compact(response)
    notif = LargeNotif(response_json, TOP_TYPE_INSTANCE, VERSION)
    receive_notifications(notif)


def get_memory(host_info):
    """get total physical memory for machine"""
    with open("/proc/meminfo") as f:
        pieces = f.readline()
        _, mem_total, _ = pieces.split()
        host_info["host_mem_total"] = mem_total

    return host_info


def get_host_info():
    """ aggregate all host info """
    host_info = {"host_metadata_version": VERSION}
    get_cpu_info(host_info)
    get_kernel_info(host_info)
    get_aws_info(host_info)
    get_collectd_version(host_info)
    get_linux_version(host_info)
    get_memory(host_info)
    get_interfaces(host_info)
    return host_info


def map_diff(host_info, old_host_info):
    """
    diff old and new host_info for additions of modifications
    don't look for removals as they will likely be spurious
    """
    diff = {}
    for k, v in iter(host_info.items()):
        if k not in old_host_info:
            diff[k] = v
        elif old_host_info[k] != v:
            diff[k] = v
    return diff


def put_val(plugin_instance, type_instance, val, plugin=PLUGIN_NAME, t=0.0,
            i=INTERVAL):
    """Create collectd metric"""

    try:
        if __name__ != "__main__":
            collectd.Values(plugin=plugin,
                            time=t,
                            plugin_instance=plugin_instance,
                            type=val[1].lower(),
                            meta={'0': True},
                            type_instance=type_instance,
                            interval=i,
                            values=[val[0]]).dispatch()
        else:
            h = platform.node()
            print('PUTVAL %s/%s/%s-%s interval=%d N:%s' % (
                h, PLUGIN_NAME, val[1].lower(),
                type_instance, INTERVAL, val[0]))
    except TypeError:
        global UTILIZATION
        UTILIZATION = False
        log("ERROR: Utilization features have been disabled because TypesDB" +
            " hasn't been specified")
        log("To use the utilization features of this plugin, please update" +
            " the top of your config to include" +
            " 'TypesDB \"/opt/signalfx-collectd-plugin/types.db.plugin\"'")
        raise


def get_uptime():
    """get uptime for plugin"""
    return time.time() - UP


def send_datapoints():
    """write proof-of-life datapoint"""
    put_val("", "sf.host-plugin_uptime", [get_uptime(), "gauge"])
    global MAX_RESPONSE
    maximum = MAX_RESPONSE
    MAX_RESPONSE = 0
    if DPM:
        global DATAPOINT_COUNT
        with RESPONSE_LOCK:
            dp = DATAPOINT_COUNT
            DATAPOINT_COUNT = {}
            diff = time.time() - LAST
            dpm = {}
            for k, v in dp.items():
                dpm[k] = int((v / diff) * 60.0)
        if dpm:
            for k, v in dpm.items():
                put_val(k, "sf.host-dpm", [v, "gauge"])

    if maximum:
        put_val("", "sf.host-response.max", [maximum, "gauge"])

    put_val("", "sf.host-response.errors", [RESPONSE_ERRORS, "counter"])


def putnotif(property_name, message, plugin_name=PLUGIN_NAME,
             type_instance=HOST_TYPE_INSTANCE, type=TYPE):
    """Create collectd notification"""
    if __name__ != "__main__":
        notif = collectd.Notification(plugin=plugin_name,
                                      plugin_instance=property_name,
                                      type_instance=type_instance,
                                      type=type)
        notif.severity = 4  # OKAY
        notif.message = message
        notif.dispatch()
    else:
        h = platform.node()
        print('PUTNOTIF %s/%s-%s/%s-%s %s' % (h, plugin_name, property_name,
                                              type, type_instance, message))


def write_notifications(host_info):
    """emit any new notifications"""
    for property_name, property_value in iter(host_info.items()):
        if len(property_value) > 255:
            receive_notifications(LargeNotif(property_value,
                                             HOST_TYPE_INSTANCE,
                                             property_name))
        else:
            putnotif(property_name, property_value)


def send_notifications():
    host_info = get_host_info()
    write_notifications(host_info)


def get_severity(severity_int):
    """
    helper meethod to swap severities

    :param severity_int: integer value for severity
    :return: collectd string for severity
    """
    return {
        1: "FAILURE",
        2: "WARNING",
        4: "OKAY"
    }[severity_int]


def update_response_times(diff):
    """
    Update max response time

    :param diff: how long a round trip took
    :return: None
    """
    with RESPONSE_LOCK:
        global MAX_RESPONSE
        if diff > MAX_RESPONSE:
            MAX_RESPONSE = diff


def steal_host_from_notifications(notif):
    """
    callback to consume notifications from collectd and steal host name from it
    even if we don't want to have the plugin send them.
    :param notif: notification
    :return: true if should continue, false if not
    """

    if not notif:
        return False

    if __name__ == "__main__":
        log(notif)
        return False

    # we send our own notifications but we don't have access to collectd's
    # "host" from collectd.conf steal it from notifications we've put on the
    # bus so we can use it for our own
    global HOST
    if not HOST and notif.host:
        HOST = notif.host
        DOGSTATSD_INSTANCE.set_host(notif.host)
        log("found host " + HOST)

    return True


def receive_notifications(notif):
    """
    callback to consume notifications from collectd and emit them to SignalFx.
    callback will only be called if Notifications was configured to be true.
    Only send notifications created by other plugs which are above or equal
    the configured NotifyLevel.
    """

    if not steal_host_from_notifications(notif):
        return

    notif_dict = {}
    # because collectd c->python is a bit limited and lacks __dict__
    for x in ['host', 'message', 'plugin', 'plugin_instance', 'severity',
              'time', 'type', 'type_instance']:
        notif_dict[x] = getattr(notif, x, "")

    # emit notifications that are ours, or satisfy the notify level
    if notif_dict['plugin'] != PLUGIN_NAME and notif_dict['type'] != TYPE \
            and notif_dict['type_instance'] not in [HOST_TYPE_INSTANCE,
                                                    TOP_TYPE_INSTANCE] \
            and notif_dict["severity"] > NOTIFY_LEVEL:
        log("event ignored: " + str(notif_dict))
        return

    if not notif_dict["time"]:
        notif_dict["time"] = time.time()
    if not notif_dict["host"]:
        if HOST:
            notif_dict["host"] = HOST
        else:
            notif_dict["host"] = platform.node()
        log("no host info, setting to " + notif_dict["host"])

    notif_dict["severity"] = get_severity(notif_dict["severity"])
    data = compact([notif_dict])
    headers = {"Content-Type": "application/json"}
    if API_TOKEN != "":
        headers["X-SF-TOKEN"] = API_TOKEN
    start = time.time()
    try:
        req = urllib2.Request(POST_URL, data, headers)
        r = urllib2.urlopen(req, timeout=TIMEOUT)
        sys.stdout.write(string.strip(r.read()))
    except Exception:
        t, e = sys.exc_info()[:2]
        sys.stdout.write(str(e))
        log("unsuccessful response: %s" % str(e))
        global RESPONSE_ERRORS
        RESPONSE_ERRORS += 1
    finally:
        diff = time.time() - start
        update_response_times(diff * 1000000.0)


def receive_datapoint(values_obj):
    """
    write callback method. Useful for DPM calculations and aggregations.

    The grab_*_metrics are listening for metrics of the type they want.
    We can only know how many disks we have after we've seen at least
    one full round of collections.  Once we have this size, we can emit on the
    last value, so we emit one interval earlier.

    :param values_obj: collectd.python Values object
    :return: None
    """
    if DPM:
        with RESPONSE_LOCK:
            global DATAPOINT_COUNT
            DATAPOINT_COUNT.setdefault(values_obj.plugin, 0)
            DATAPOINT_COUNT[values_obj.plugin] += len(values_obj.values)

    if not UTILIZATION:
        return

    with METRIC_LOCK:
        if values_obj.plugin == "aggregation" and values_obj.type \
                == "cpu" and values_obj.plugin_instance == "cpu-average":
            grab_cpu_metrics(values_obj)
        elif values_obj.plugin == "memory" and values_obj.type == "memory":
            grab_memory_metrics(values_obj)
        elif values_obj.plugin == "df" and values_obj.type == "df_complex":
            grab_df_metrics(values_obj)

        elif values_obj.plugin == "interface" \
                and values_obj.type == "if_octets":
            grab_network_metrics(values_obj)
        elif values_obj.plugin == "disk" and values_obj.type == "disk_ops":
            grab_disk_io_metrics(values_obj)


def grab_cpu_metrics(values_obj):
    """
    Grabs cpu metrics from write callback.

    :param values_obj: collectd Values object
    :return: None
    """
    metric = values_obj.type + "." + values_obj.type_instance
    ti = round(values_obj.time, 1)
    global CPU_HISTORY, CPU_DONE
    if CPU_HISTORY and ti not in CPU_HISTORY:
        debug("incomplete cpu, skipping %s" % CPU_HISTORY)
        CPU_HISTORY = {}  # skip it because it's incomplete
    cpu_time = CPU_HISTORY.setdefault(ti, {})
    cpu_time[metric] = values_obj.values
    if len(cpu_time) == 8:
        CPU_DONE.append((ti, cpu_time))
        CPU_HISTORY = {}


def grab_memory_metrics(values_obj):
    """
    Grabs memory metrics from write callback.

    :param values_obj: collectd Values object
    :return: None
    """
    metric = values_obj.type + "." + values_obj.type_instance
    ti = round(values_obj.time, 1)
    global MEMORY_HISTORY, MEMORY_DONE, MAX_MEMORY_LENGTH
    if ti not in MEMORY_HISTORY:
        for t in MEMORY_HISTORY:
            debug("memory timings %s %s" % (ti, t))
            if MAX_MEMORY_LENGTH < len(MEMORY_HISTORY[t]):
                debug("setting max length memory %s"
                      % len(MEMORY_HISTORY[t]))
                MAX_MEMORY_LENGTH = len(MEMORY_HISTORY[t])
            if MAX_MEMORY_LENGTH == len(MEMORY_HISTORY[t]):
                debug("appending memory at top %s %s"
                      % (t, MEMORY_HISTORY[t]))
                MEMORY_DONE.append((t, MEMORY_HISTORY[t]))
            else:
                debug("incomplete memory, skipping %s %s"
                      % (t, MEMORY_HISTORY[t]))

        MEMORY_HISTORY = {}
    mem_time = MEMORY_HISTORY.setdefault(ti, {})
    mem_time[metric] = values_obj.values
    if MAX_MEMORY_LENGTH and len(mem_time) \
            >= MAX_MEMORY_LENGTH and "memory.free" in mem_time:
        MEMORY_DONE.append((ti, mem_time))
        debug("sending MEMORY %s %s" % (ti, mem_time))
        MEMORY_HISTORY = {}


def grab_df_metrics(values_obj):
    """
    Grabs df metrics from write callback.

    :param values_obj: collectd Values object
    :return: None
    """
    metric = values_obj.type + "." + values_obj.type_instance
    ti = round(values_obj.time, 1)
    global DF_HISTORY, DF_DONE, MAX_DF_LENGTH
    if ti not in DF_HISTORY:
        for t in DF_HISTORY:
            if MAX_DF_LENGTH < len(DF_HISTORY[t]):
                MAX_DF_LENGTH = len(DF_HISTORY[t])
                debug("setting DF_MAX_LENGTH %s" % len(DF_HISTORY[t]))
            if MAX_DF_LENGTH == len(DF_HISTORY[t]):
                debug("appending df at top %s" % DF_HISTORY[t])
                DF_DONE.append((t, DF_HISTORY[t]))
            else:
                debug("incomplete df, skipping %s %s"
                      % (t, DF_HISTORY[t]))
        DF_HISTORY = {}
    disk_time = DF_HISTORY.setdefault(ti, {})
    metric_history = \
        disk_time.setdefault(values_obj.plugin_instance, {})
    metric_history[metric] = values_obj.values
    if len(metric_history) == 3 and MAX_DF_LENGTH \
            and len(disk_time) == MAX_DF_LENGTH:
        good = True
        for t in disk_time:
            good = good and len(disk_time[t]) == 3
        if good:
            debug("appending df at bottom %s" % disk_time)
            DF_DONE.append((ti, disk_time))
            DF_HISTORY = {}


def grab_network_metrics(values_obj):
    """
    Grabs network metrics from write callback.

    :param values_obj: collectd Values object
    :return: None
    """
    ti = round(values_obj.time, 1)
    global NETWORK_HISTORY, NETWORK_DONE, MAX_NETWORK_LENGTH
    if ti not in NETWORK_HISTORY:
        for t in NETWORK_HISTORY:
            if MAX_NETWORK_LENGTH < len(NETWORK_HISTORY[t]):
                debug("setting max length network %s"
                      % len(NETWORK_HISTORY[t]))
                MAX_NETWORK_LENGTH = len(NETWORK_HISTORY[t])
            if MAX_NETWORK_LENGTH == len(NETWORK_HISTORY[t]):
                NETWORK_DONE.append((t, NETWORK_HISTORY[t]))
                debug("network appended at top %s" % NETWORK_HISTORY[t])
            else:
                debug("incomplete network, skipping %s %s"
                      % (t, NETWORK_HISTORY[t]))
        NETWORK_HISTORY = {}
    network_time = NETWORK_HISTORY.setdefault(ti, {})
    metric_history = \
        network_time.setdefault(values_obj.plugin_instance, {})
    metric_history[values_obj.type] = values_obj.values
    if MAX_NETWORK_LENGTH and len(network_time) == MAX_NETWORK_LENGTH:
        debug("appending network at bottom %s" % network_time)
        NETWORK_DONE.append((ti, network_time))
        NETWORK_HISTORY = {}


def grab_disk_io_metrics(values_obj):
    """
    Grabs disk io metrics from write callback.

    :param values_obj: collectd Values object
    :return: None
    """

    ti = round(values_obj.time, 1)
    global DISK_IO_HISTORY, DISK_IO_DONE, MAX_DISK_IO_LENGTH
    if DISK_IO_HISTORY and ti not in DISK_IO_HISTORY:
        for t in DISK_IO_HISTORY:
            debug("disk io timings %s %s" % (ti, t))
            if MAX_DISK_IO_LENGTH < len(DISK_IO_HISTORY[t]):
                debug("setting max length disk io %s %s"
                      % (t, len(DISK_IO_HISTORY[t])))
                MAX_DISK_IO_LENGTH = len(DISK_IO_HISTORY[t])
            if MAX_DISK_IO_LENGTH == len(DISK_IO_HISTORY[t]):
                DISK_IO_DONE.append((t, DISK_IO_HISTORY[t]))
                debug("disk io appended at top length %s t %s %s" %
                      (len(DISK_IO_HISTORY[t]), t, DISK_IO_HISTORY[t]))
            else:
                debug("disk io not appending at top length %s t %s %s" %
                      (len(DISK_IO_HISTORY[t]), t, DISK_IO_HISTORY[t]))

        if t != ti:
            DISK_IO_HISTORY = {}
    disk_time = DISK_IO_HISTORY.setdefault(ti, {})
    metric_history = \
        disk_time.setdefault(values_obj.plugin_instance, {})
    metric_history[values_obj.type] = values_obj.values
    if MAX_DISK_IO_LENGTH and len(disk_time) == MAX_DISK_IO_LENGTH:
        debug("appending disk io at bottom %s %s" % (ti, disk_time))
        DISK_IO_DONE.append((ti, disk_time))
        DISK_IO_HISTORY = {}


def restore_sigchld():
    """
    Restores the SIGCHLD handler if needed

    See https://github.com/deniszh/collectd-iostat-python/issues/2 for
    details.
    """
    try:
        platform.system()
    except:
        log("executing SIGCHLD workaround")
        signal.signal(signal.SIGCHLD, signal.SIG_DFL)
    if __name__ != "__main__":
        DOGSTATSD_INSTANCE.init_callback()


def log_cb(severity, message):
    if "Value too old" in message:
        global DEBUG
        if not DEBUG:
            log("turning on DEBUG due to error message")
            DEBUG = True


# Note: Importing collectd_dogstatsd registers its own endpoints

if __name__ != "__main__":
    # when running inside plugin
    collectd.register_read(emit_utilizations, 1, name="utilization_reads")
    collectd.register_notification(steal_host_from_notifications)
    collectd.register_init(restore_sigchld)
    collectd.register_config(plugin_config)
    collectd.register_shutdown(DOGSTATSD_INSTANCE.register_shutdown)
    collectd.register_log(log_cb)

else:
    # outside plugin just collect the info
    restore_sigchld()
    send()
    log(json.dumps(get_host_info(), sort_keys=True,
                   indent=4, separators=(',', ': ')))
    if len(sys.argv) < 2:
        while True:
            time.sleep(INTERVAL)
            send()
